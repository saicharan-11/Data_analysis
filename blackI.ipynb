{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import re\n",
    "import urllib\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_excel(r'C:\\Users\\pinup\\Downloads\\cik_list.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['SECFNAME']='https://www.sec.gov/Archives/'+df['SECFNAME'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words=[]\n",
    "f=open(r\"C:\\Users\\pinup\\Downloads\\StopWords_GenericLong.txt\",'r')\n",
    "for i in f:\n",
    "    i=i.strip()\n",
    "    stop_words.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p=pd.read_excel(r\"C:\\Users\\pinup\\Downloads\\LoughranMcDonald_SentimentWordLists_2018.xlsx\",sheet_name='Positive',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_N=pd.read_excel(r\"C:\\Users\\pinup\\Downloads\\LoughranMcDonald_SentimentWordLists_2018.xlsx\",sheet_name='Negative',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p.rename( columns={0:'postives'}, inplace=True )\n",
    "df_N.rename(columns={0:'negatives'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_words=df_p['postives'].tolist()\n",
    "negative_words=df_N['negatives'].tolist()\n",
    "positive_words=[w.lower() for w in positive_words]\n",
    "negative_words=[w.lower() for w in negative_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filtering(link):\n",
    "    matter=[]\n",
    "    mg=[]\n",
    "    qc=[]\n",
    "    rf=[]\n",
    "    i=None\n",
    "    j=None\n",
    "    p=None\n",
    "    file = urllib.request.urlopen(link)\n",
    "    for line in file:\n",
    "        decoded_line = line.decode(\"utf-8\")\n",
    "        clean=re.compile('<.*?>')\n",
    "        clean_line=re.sub(clean,'',decoded_line)\n",
    "        matter.append(clean_line)\n",
    "    for index,cont in enumerate(matter):\n",
    "        r=cont.find(\"MANAGEMENT'S DISCUSSION\")\n",
    "        t=cont.find(\"QUANTITATIVE AND QUALITATIVE\")\n",
    "        e=cont.find(\"RISK FACTORS\")\n",
    "        if r!=-1:\n",
    "            i=index\n",
    "        if t!=-1:\n",
    "            j=index\n",
    "        if e!=-1:\n",
    "            p=index\n",
    "    if i!=None:\n",
    "        for k in range(i,len(matter)-1):\n",
    "            g=matter[k+1].find(\"ITEM\")\n",
    "            mg.append(matter[k])\n",
    "            if g!=-1:\n",
    "                break\n",
    "    if j!=None:\n",
    "        for k in range(j,len(matter)-1):\n",
    "            g=matter[k+1].find(\"ITEM\")\n",
    "            qc.append(matter[k])\n",
    "            if g!=-1:\n",
    "                break \n",
    "    if p!=None:\n",
    "        for k in range(p,len(matter)-1):\n",
    "            g=matter[k+1].find(\"ITEM\")\n",
    "            rf.append(matter[k])\n",
    "            if g!=-1:\n",
    "                break\n",
    "    return(mg,qc,rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing(m,q,r):\n",
    "    pos_m=0\n",
    "    pos_q=0\n",
    "    pos_r=0\n",
    "    neg_m=0\n",
    "    neg_q=0\n",
    "    neg_r=0\n",
    "    required_m=[]\n",
    "    required_q=[]\n",
    "    required_r=[]\n",
    "    pos_prop_m=[]\n",
    "    pos_prop_q=[]\n",
    "    pos_prop_r=[]\n",
    "    neg_prop_m=[]\n",
    "    neg_prop_q=[]\n",
    "    neg_prop_r=[]\n",
    "    str_m=''.join(m)\n",
    "    str_q=''.join(q)\n",
    "    str_r=''.join(r)\n",
    "    tokens_m=word_tokenize(str_m)\n",
    "    tokens_q=word_tokenize(str_q)\n",
    "    tokens_r=word_tokenize(str_r)\n",
    "    tokens_m= [word for word in tokens_m if word.isalnum()]\n",
    "    tokens_q= [word for word in tokens_q if word.isalnum()]\n",
    "    tokens_r= [word for word in tokens_r if word.isalnum()]\n",
    "    tokens_m = [w.lower() for w in tokens_m]\n",
    "    tokens_q = [w.lower() for w in tokens_q]\n",
    "    tokens_r = [w.lower() for w in tokens_r]\n",
    "    for i in tokens_m:\n",
    "        if i in stop_words:\n",
    "            pass\n",
    "        else:\n",
    "            required_m.append(i)\n",
    "            \n",
    "    for i in tokens_q:\n",
    "        if i in stop_words:\n",
    "            pass\n",
    "        else:\n",
    "            required_q.append(i)\n",
    "    for i in tokens_r:\n",
    "        if i in stop_words:\n",
    "            pass\n",
    "        else:\n",
    "            required_r.append(i)\n",
    "    m_score=[]\n",
    "    q_score=[]\n",
    "    r_score=[]\n",
    "    for i in required_m:\n",
    "        if i in positive_words:\n",
    "            pos_m=pos_m+1\n",
    "        if i in negative_words:\n",
    "            neg_m=neg_m-1\n",
    "    m_score.append((pos_m,neg_m*(-1),len(required_m)))                 \n",
    "    for i in required_q:\n",
    "        if i in positive_words:\n",
    "            pos_q=pos_q+1\n",
    "        if i in negative_words:\n",
    "            neg_q=neg_q-1\n",
    "    q_score.append((pos_q,neg_q*(-1),len(required_q)))      \n",
    "    for i in required_r:\n",
    "        if i in positive_words:\n",
    "            pos_r=pos_r+1\n",
    "        if i in negative_words:\n",
    "            neg_r=neg_r-1\n",
    "    r_score.append((pos_r,neg_r*(-1),len(required_r)))\n",
    "    m_polarity=[]\n",
    "    q_polarity=[]\n",
    "    r_polarity=[]\n",
    "    for i in range(len(m_score)):\n",
    "        try:\n",
    "            for i in m_score:\n",
    "                m_polarity.append((i[0]-i[1])/((i[0]+i[1])+0.000001))\n",
    "        except ZeroDivisionError:\n",
    "            m_polarity.append(0)\n",
    "    for i in range(len(q_score)):\n",
    "        try:\n",
    "            for i in q_score:\n",
    "                q_polarity.append((i[0]-i[1])/((i[0]+i[1])+0.000001))\n",
    "        except ZeroDivisionError:\n",
    "             q_polarity.append(0)\n",
    "    for i in range(len(r_score)):\n",
    "        try:\n",
    "            for i in r_score:\n",
    "                r_polarity.append((i[0]-i[1])/((i[0]+i[1])+0.000001))\n",
    "        except ZeroDivisionError:\n",
    "             r_polarity.append(0)\n",
    "    m_subjectivity=[]\n",
    "    q_subjectivity=[]\n",
    "    r_subjectivity=[]\n",
    "    for i in range(len(m_score)):\n",
    "        try:\n",
    "            for i in m_score:\n",
    "                m_subjectivity.append((i[0] + i[1])/(i[2]+0.000001))\n",
    "        except ZeroDivisionError:\n",
    "            m_subjectivity.append(0)\n",
    "            \n",
    "    for i in range(len(q_score)):\n",
    "        try:\n",
    "            for i in q_score:\n",
    "                q_subjectivity.append((i[0] + i[1])/(i[2]+0.000001))\n",
    "        except ZeroDivisionError:\n",
    "            q_subjectivity.append(0)\n",
    "            \n",
    "    for i in range(len(r_score)):\n",
    "        try:\n",
    "            for i in r_score:\n",
    "                r_subjectivity.append((i[0] + i[1])/(i[2]+0.000001))\n",
    "        except ZeroDivisionError:\n",
    "            r_subjectivity.append(0)\n",
    "    for i in range(len(m_score)):\n",
    "        try:\n",
    "            for i in m_score:\n",
    "                pos_prop_m.append(i[0]/len(tokens_m))\n",
    "        except:\n",
    "            pos_prop_m.append(0)\n",
    "    for i in range(len(q_score)):\n",
    "        try:\n",
    "            for i in q_score:\n",
    "                pos_prop_q.append(i[0]/len(tokens_q))\n",
    "        except:\n",
    "            pos_prop_q.append(0)\n",
    "    for i in range(len(r_score)):\n",
    "        try:\n",
    "            for i in r_score:\n",
    "                pos_prop_r.append(i[0]/len(tokens_r))\n",
    "        except:\n",
    "            pos_prop_r.append(0)\n",
    "            \n",
    "    for i in range(len(m_score)):\n",
    "        try:\n",
    "            for i in m_score:\n",
    "                neg_prop_m.append(i[1]/len(tokens_m))\n",
    "        except:\n",
    "            neg_prop_m.append(0)\n",
    "    for i in range(len(q_score)):\n",
    "        try:\n",
    "            for i in q_score:\n",
    "                neg_prop_q.append(i[1]/len(tokens_q))\n",
    "        except:\n",
    "            neg_prop_q.append(0)\n",
    "    for i in range(len(r_score)):\n",
    "        try:\n",
    "            for i in r_score:\n",
    "                neg_prop_r.append(i[1]/len(tokens_r))\n",
    "        except:\n",
    "            neg_prop_r.append(0)\n",
    "    return(m_score,q_score,r_score,m_polarity,q_polarity,r_polarity,pos_prop_m,pos_prop_q,pos_prop_r,neg_prop_m,neg_prop_q,neg_prop_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_sentence(m,q,r):\n",
    "    str_m=''.join(m)\n",
    "    str_q=''.join(q)\n",
    "    str_r=''.join(r)\n",
    "    tokens_m=word_tokenize(str_m)\n",
    "    tokens_q=word_tokenize(str_q)\n",
    "    tokens_r=word_tokenize(str_r)\n",
    "    tokens_m= [word for word in tokens_m if word.isalpha()]\n",
    "    tokens_q= [word for word in tokens_q if word.isalpha()]\n",
    "    tokens_r= [word for word in tokens_r if word.isalpha()]\n",
    "    tokens_sm=sent_tokenize(str_m)\n",
    "    tokens_sq=sent_tokenize(str_q)\n",
    "    tokens_sr=sent_tokenize(str_r)\n",
    "    try:\n",
    "        avg_sen_m=len(tokens_m)/len(tokens_sm)\n",
    "    except:\n",
    "        avg_sen_m=0\n",
    "    try:\n",
    "        avg_sen_q=len(tokens_q)/len(tokens_sq)\n",
    "    except:\n",
    "        avg_sen_q=0\n",
    "    try:\n",
    "        avg_sen_r=len(tokens_r)/len(tokens_sr)\n",
    "    except:\n",
    "        avg_sen_r=0\n",
    "    return(avg_sen_m,avg_sen_q,avg_sen_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def per_complex_words(m,q,r):\n",
    "    count=0\n",
    "    m_count=0\n",
    "    q_count=0\n",
    "    r_count=0\n",
    "    vowels=['a','e','i','o','u']\n",
    "    str_m=''.join(m)\n",
    "    str_q=''.join(q)\n",
    "    str_r=''.join(r)\n",
    "    tokens_m=word_tokenize(str_m)\n",
    "    tokens_q=word_tokenize(str_q)\n",
    "    tokens_r=word_tokenize(str_r)\n",
    "    tokens_m= [word for word in tokens_m if word.isalnum()]\n",
    "    tokens_q= [word for word in tokens_q if word.isalnum()]\n",
    "    tokens_r= [word for word in tokens_r if word.isalnum()]\n",
    "    tokens_m = [w.lower() for w in tokens_m]\n",
    "    tokens_q = [w.lower() for w in tokens_q]\n",
    "    tokens_r = [w.lower() for w in tokens_r]\n",
    "    for i in tokens_m:\n",
    "        if i.endswith('ed') or i.endswith('es'):\n",
    "            pass\n",
    "        else:\n",
    "            for j in range(len(i)):\n",
    "                if i[j] in vowels:\n",
    "                    count+=1\n",
    "                    if count>2:\n",
    "                        m_count+=1\n",
    "                        count=0\n",
    "    count=0\n",
    "    for i in tokens_q:\n",
    "        if i.endswith('ed') or i.endswith('es'):\n",
    "            pass\n",
    "        else:\n",
    "            for j in range(len(i)):\n",
    "                if i[j] in vowels:\n",
    "                    count+=1\n",
    "                    if count>2:\n",
    "                        q_count+=1\n",
    "                        count=0\n",
    "    count=0\n",
    "    for i in tokens_r:\n",
    "        if i.endswith('ed') or i.endswith('es'):\n",
    "            pass\n",
    "        else:\n",
    "            for j in range(len(i)):\n",
    "                if i[j] in vowels:\n",
    "                    count+=1\n",
    "                    if count>2:\n",
    "                        r_count+=1\n",
    "                        count=0\n",
    "    try:\n",
    "        per_m=m_count/len(tokens_m)\n",
    "    except:\n",
    "        per_m=0\n",
    "    try:\n",
    "        per_q=q_count/len(tokens_q)\n",
    "    except:\n",
    "        per_q=0\n",
    "    try:\n",
    "        per_r=r_count/len(tokens_r)\n",
    "    except:\n",
    "        per_r=0\n",
    "    return(len(tokens_m),len(tokens_q),len(tokens_r),m_count,q_count,r_count,per_m,per_q,per_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fog_index(avg_sen,per_com):\n",
    "    fog_m=0.4*(avg_sen[0]+per_com[0])\n",
    "    fog_q=0.4*(avg_sen[1]+per_com[1])\n",
    "    fog_r=0.4*(avg_sen[2]+per_com[2])    \n",
    "    return(fog_m,fog_q,fog_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_un=pd.read_excel(r'C:\\Users\\pinup\\Downloads\\uncertainty_dictionary.xlsx')\n",
    "df_con=pd.read_excel(r'C:\\Users\\pinup\\Downloads\\constraining_dictionary.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "un_words=df_un['Word'].tolist()\n",
    "con_words=df_con['Word'].tolist()\n",
    "un_words=[w.lower() for w in un_words]\n",
    "con_words=[w.lower() for w in con_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def un_certainty(m,q,r):\n",
    "    count_m=0\n",
    "    count_q=0\n",
    "    count_r=0\n",
    "    un_prop_m=0\n",
    "    un_prop_q=0\n",
    "    un_prop_r=0\n",
    "    str_m=''.join(m)\n",
    "    str_q=''.join(q)\n",
    "    str_r=''.join(r)\n",
    "    tokens_m=word_tokenize(str_m)\n",
    "    tokens_q=word_tokenize(str_q)\n",
    "    tokens_r=word_tokenize(str_r)\n",
    "    tokens_m= [word for word in tokens_m if word.isalnum()]\n",
    "    tokens_q= [word for word in tokens_q if word.isalnum()]\n",
    "    tokens_r= [word for word in tokens_r if word.isalnum()]\n",
    "    tokens_m = [w.lower() for w in tokens_m]\n",
    "    tokens_q = [w.lower() for w in tokens_q]\n",
    "    tokens_r = [w.lower() for w in tokens_r]\n",
    "    for i in tokens_m:\n",
    "        if i in un_words:\n",
    "            count_m+=1\n",
    "    for i in tokens_q:\n",
    "        if i in un_words:\n",
    "            count_q+=1\n",
    "    for i in tokens_r:\n",
    "        if i in un_words:\n",
    "            count_r+=1         \n",
    "    try:\n",
    "        un_prop_m=count_m/(len(tokens_m))\n",
    "    except:\n",
    "        un_prop_m=0\n",
    "    try:\n",
    "        un_prop_q=count_q/(len(tokens_q))\n",
    "    except:\n",
    "        un_prop_q=0\n",
    "    try:\n",
    "        un_prop_r=count_r/(len(tokens_r))\n",
    "    except:\n",
    "        un_prop_r=0\n",
    "    return(count_m,count_q,count_r,un_prop_m,un_prop_q,un_prop_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def constrain(m,q,r):\n",
    "    count_m=0\n",
    "    count_q=0\n",
    "    count_r=0\n",
    "    str_m=''.join(m)\n",
    "    str_q=''.join(q)\n",
    "    str_r=''.join(r)\n",
    "    tokens_m=word_tokenize(str_m)\n",
    "    tokens_q=word_tokenize(str_q)\n",
    "    tokens_r=word_tokenize(str_r)\n",
    "    tokens_m= [word for word in tokens_m if word.isalnum()]\n",
    "    tokens_q= [word for word in tokens_q if word.isalnum()]\n",
    "    tokens_r= [word for word in tokens_r if word.isalnum()]\n",
    "    tokens_m = [w.lower() for w in tokens_m]\n",
    "    tokens_q = [w.lower() for w in tokens_q]\n",
    "    tokens_r = [w.lower() for w in tokens_r]\n",
    "    for i in tokens_m:\n",
    "        if i in con_words:\n",
    "            count_m+=1\n",
    "    for i in tokens_q:\n",
    "        if i in con_words:\n",
    "            count_q+=1\n",
    "    for i in tokens_r:\n",
    "        if i in con_words:\n",
    "            count_r+=1\n",
    "    try:\n",
    "        con_prop_m=count_m/(len(tokens_m))\n",
    "    except:\n",
    "        con_prop_m=0\n",
    "    try:\n",
    "        con_prop_q=count_q/(len(tokens_q))\n",
    "    except:\n",
    "        con_prop_q=0\n",
    "    try:\n",
    "        con_prop_r=count_r/(len(tokens_r))\n",
    "    except:\n",
    "        con_prop_r=0\n",
    "    return(count_m,count_q,count_r,con_prop_m,con_prop_q,con_prop_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w_uncer_cons(link):\n",
    "    count_un=0\n",
    "    count_con=0\n",
    "    file = urllib.request.urlopen(link)\n",
    "    for line in file:\n",
    "        decoded_line = line.decode(\"utf-8\")\n",
    "        clean=re.compile('<.*?>')\n",
    "        clean_line=re.sub(clean,'',decoded_line)\n",
    "        tokens=word_tokenize(clean_line)\n",
    "        tokens= [word for word in tokens if word.isalnum()]\n",
    "        tokens = [w.lower() for w in tokens]\n",
    "        for i in tokens:\n",
    "            if i in un_words:\n",
    "                count_un+=1\n",
    "            if i in con_words:\n",
    "                count_con+=1\n",
    "    return(count_un,count_con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_positive=[]\n",
    "q_positive=[]\n",
    "r_positive=[]\n",
    "m_negative=[]\n",
    "q_negative=[]\n",
    "r_negative=[]\n",
    "m_polarity=[]\n",
    "q_polarity=[]\n",
    "r_polarity=[]\n",
    "m_avg_sent=[]\n",
    "q_avg_sent=[]\n",
    "r_avg_sent=[]\n",
    "m_per_comp=[]\n",
    "q_per_comp=[]\n",
    "r_per_comp=[]\n",
    "m_fog_ind=[]\n",
    "q_fog_ind=[]\n",
    "r_fog_ind=[]\n",
    "m_com_count=[]\n",
    "q_com_count=[]\n",
    "r_com_count=[]\n",
    "m_word_count=[]\n",
    "q_word_count=[]\n",
    "r_word_count=[]\n",
    "m_uncertain=[]\n",
    "q_uncertain=[]\n",
    "r_uncertain=[]\n",
    "m_constrain=[]\n",
    "q_constrain=[]\n",
    "r_constrain=[]\n",
    "w_uncertain=[]\n",
    "w_constrain=[]\n",
    "m_pos_proportion=[]\n",
    "q_pos_proportion=[]\n",
    "r_pos_proportion=[]\n",
    "m_neg_proportion=[]\n",
    "q_neg_proportion=[]\n",
    "r_neg_proportion=[]\n",
    "m_un_proportion=[]\n",
    "q_un_proportion=[]\n",
    "r_un_proportion=[]\n",
    "m_con_proportion=[]\n",
    "q_con_proportion=[]\n",
    "r_con_proportion=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df['SECFNAME']:\n",
    "    z=filtering(i)\n",
    "    pro=processing(z[0],z[1],z[2])\n",
    "    a=avg_sentence(z[0],z[1],z[2])\n",
    "    p=per_complex_words(z[0],z[1],z[2])\n",
    "    f=fog_index(a,p)\n",
    "    u=un_certainty(z[0],z[1],z[2])\n",
    "    c=constrain(z[0],z[1],z[2])\n",
    "    unc=w_uncer_cons(i)\n",
    "    pro_mp=pro[0][0][0]\n",
    "    pro_qp=pro[1][0][0]\n",
    "    pro_rp=pro[2][0][0]\n",
    "    pro_mn=pro[0][0][1]\n",
    "    pro_qn=pro[1][0][1]\n",
    "    pro_rn=pro[2][0][1]\n",
    "    pro_mpol=pro[3][0]\n",
    "    pro_qpol=pro[4][0]\n",
    "    pro_rpol=pro[5][0]\n",
    "    avg_sen_m=a[0]\n",
    "    avg_sen_q=a[1]\n",
    "    avg_sen_r=a[2]\n",
    "    m_positive.append(pro_mp)\n",
    "    q_positive.append(pro_qp)\n",
    "    r_positive.append(pro_rp)\n",
    "    m_negative.append(pro_mn)\n",
    "    q_negative.append(pro_qn)\n",
    "    r_negative.append(pro_rn)\n",
    "    m_polarity.append(pro_mpol)\n",
    "    q_polarity.append(pro_qpol)\n",
    "    r_polarity.append(pro_rpol)\n",
    "    m_avg_sent.append(avg_sen_m)\n",
    "    q_avg_sent.append(avg_sen_q)\n",
    "    r_avg_sent.append(avg_sen_r)\n",
    "    m_word_count.append(p[0])\n",
    "    q_word_count.append(p[1])\n",
    "    r_word_count.append(p[2])\n",
    "    m_com_count.append(p[3])\n",
    "    q_com_count.append(p[4])\n",
    "    r_com_count.append(p[5])\n",
    "    m_per_comp.append(p[6])\n",
    "    q_per_comp.append(p[7])\n",
    "    r_per_comp.append(p[8])\n",
    "    m_fog_ind.append(f[0])\n",
    "    q_fog_ind.append(f[1])\n",
    "    r_fog_ind.append(f[2])\n",
    "    m_uncertain.append(u[0])\n",
    "    q_uncertain.append(u[1])\n",
    "    r_uncertain.append(u[2])\n",
    "    m_constrain.append(c[0])\n",
    "    q_constrain.append(c[1])\n",
    "    r_constrain.append(c[2])\n",
    "    w_uncertain.append(unc[0])\n",
    "    w_constrain.append(unc[1])\n",
    "    m_pos_proportion.append(pro[6][0])\n",
    "    q_pos_proportion.append(pro[7][0])\n",
    "    r_pos_proportion.append(pro[8][0])\n",
    "    m_neg_proportion.append(pro[9][0])\n",
    "    q_neg_proportion.append(pro[10][0])\n",
    "    r_neg_proportion.append(pro[11][0])\n",
    "    m_un_proportion.append(u[3])\n",
    "    q_un_proportion.append(u[4])\n",
    "    r_un_proportion.append(u[5])\n",
    "    m_con_proportion.append(c[3])\n",
    "    q_con_proportion.append(c[3])\n",
    "    r_con_proportion.append(c[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "di=df['SECFNAME'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "output={'cik_list': di,'mda_positive_score': m_positive,'mda_negative_score': m_negative,'mda_polarity_score': m_polarity,'mda_average_sentence_length': m_avg_sent,'mda_percentage_of_complex_words': m_per_comp,'mda_fog_index': m_fog_ind,'mda_complex_word_count': m_com_count,'mda_word_count': m_word_count,'mda_uncertainty_score': m_uncertain,'mda_constraining_score': m_constrain,'mda_positive_word_proportion': m_pos_proportion,'mda_negative_word_proportion': m_neg_proportion,'mda_uncertainty_word_proportion': m_un_proportion,'mda_constraining_word_proportion': m_con_proportion,'qqdmr_positive_score': q_positive,'qqdmr_negative_score': q_negative,'qqdmr_polarity_score': q_polarity,'qqdmr_average_sentence_length':q_avg_sent,'qqdmr_percentage_of_complex_words':q_per_comp,'qqdmr_fog_index':q_fog_ind,'qqdmr_complex_word_count':q_com_count,'qqdmr_word_count':q_word_count,'qqdmr_uncertainty_score':q_uncertain,'qqdmr_constraining_score':q_constrain,'qqdmr_positive_word_proportion':q_pos_proportion,'qqdmr_negative_word_proportion':q_neg_proportion,'qqdmr_uncertainty_word_proportion':q_un_proportion,'qqdmr_constraining_word_proportion':q_con_proportion,'rf_positive_score':r_positive,'rf_negative_score':r_negative,'rf_polarity_score':r_polarity,'rf_average_sentence_length':r_avg_sent,'rf_percentage_of_complex_words':r_per_comp,'rf_fog_index':r_fog_ind,'rf_complex_word_count':r_com_count,'rf_word_count':r_word_count,'rf_uncertainty_score':r_uncertain,'rf_constraining_score':r_constrain,'rf_positive_word_proportion':r_pos_proportion,'rf_negative_word_proportion':r_neg_proportion,'rf_uncertainty_word_proportion':r_un_proportion,'rf_constraining_word_proportion':r_con_proportion,'constraining_words_whole_report':w_constrain}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_pd=pd.DataFrame(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_pd.to_csv('output_dict.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
